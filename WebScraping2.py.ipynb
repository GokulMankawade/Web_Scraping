{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a35bed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b966217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save!!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "data = []\n",
    "\n",
    "base_url = \"https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_{}\"\n",
    "num_pages = 20\n",
    "\n",
    "# Initialize lists to store scraped data\n",
    "product_urls = []\n",
    "product_names = []\n",
    "product_prices = []\n",
    "ratings = []\n",
    "review_counts = []\n",
    "\n",
    "# Loop through each page\n",
    "for page in range(1, num_pages + 1):\n",
    "    url = base_url.format(page)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract product information\n",
    "    products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "\n",
    "    for product in products:\n",
    "        # Product URL\n",
    "        product_url = product.find('a', class_='a-link-normal', href=True)['href']\n",
    "        product_urls.append(\"https://www.amazon.in\" + product_url)\n",
    "\n",
    "        # Product Name\n",
    "        product_name = product.find('span', class_='a-text-normal').get_text()\n",
    "        product_names.append(product_name)\n",
    "\n",
    "        # Product Price\n",
    "        product_price = product.find('span', class_='a-price-whole').get_text()\n",
    "        product_prices.append(product_price)\n",
    "\n",
    "        # Rating\n",
    "        rating = product.find('span', class_='a-icon-alt')\n",
    "        if rating:\n",
    "            ratings.append(rating.get_text())\n",
    "        else:\n",
    "            ratings.append(\"Not available\")\n",
    "\n",
    "        # Number of Reviews\n",
    "        review_count = product.find('span', {'class': 'a-size-base', 'dir': 'auto'})\n",
    "        if review_count:\n",
    "            review_counts.append(review_count.get_text())\n",
    "        else:\n",
    "            review_counts.append(\"0\")\n",
    "\n",
    "  #---------------------------PART-2-----------------------------------#\n",
    "\n",
    "descriptions = []\n",
    "asins = []\n",
    "product_descriptions = []\n",
    "manufacturers = []\n",
    "\n",
    "# Loop through each product URL\n",
    "for product_url in product_urls:\n",
    "    response = requests.get(product_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract Description\n",
    "    description = soup.find_all('ul', class_='a-unordered-list a-vertical a-spacing-mini')\n",
    "    descriptions.append(description)\n",
    "        \n",
    "    # Extrct ASIN \n",
    "    asin1 = product_url.split('/')\n",
    "    \n",
    "    if asin_index != asin1.index('') + 1:\n",
    "        pass\n",
    "    asin = asin1[asin_index]\n",
    "\n",
    "    \n",
    "    # Extract Product Description\n",
    "    product_description = soup.find('div', {'id': 'productDescription'})\n",
    "    \n",
    "    #Extract Manufacturer\n",
    "    manufact = soup.find('div', {'id': 'detailBullets_feature_div'})\n",
    "    manufacturers.append(manufact)\n",
    "\n",
    "asins.append(asin)\n",
    "\n",
    "data.append([product_urls,product_names,product_prices,ratings,review_counts,descriptions,asins,product_descriptions,manufacturers])\n",
    "    \n",
    "    \n",
    "print(f\"Save!!\")\n",
    "    \n",
    "df = pd.DataFrame(data, columns=['product_urls', 'product_names', 'product_prices', 'ratings', 'review_counts','descriptions',\n",
    "        'asins','product_descriptions','manufacturers']) \n",
    "df.to_csv('Allpart.csv', index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7bbb2086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a219e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc513a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cfb650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c18151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0280e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef40a00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbff3ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b77d1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6392ad44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
